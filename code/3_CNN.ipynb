{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "NLP_project_Group5_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0104c8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937e6aeb-db28-4ab4-ca0c-b50590dac8bb"
      },
      "source": [
        "#Importing all the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "b0104c8a",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QloW-EeQWese",
        "outputId": "331ab7bd-6267-4f0b-ea5e-63bbdce9a7e9"
      },
      "source": [
        "#Downloading NLTK libraries\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "id": "QloW-EeQWese",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulAExa4MtDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c33f9eb-1dbe-4572-a736-ec5695cc3c0a"
      },
      "source": [
        "# Use cuda if present\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "id": "8ulAExa4MtDl",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOzBU-ODW9T"
      },
      "source": [
        "#Root folder for data files (for storage and retrieval)\n",
        "GDRIVE_PROJECT_FOLDER = '/content/gdrive/MyDrive/NLP_Project/'"
      ],
      "id": "hIOzBU-ODW9T",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R8N0p_mSbg_",
        "outputId": "207d09fd-508e-4e5c-8c95-4641cebeef0c"
      },
      "source": [
        "#Mount the google drive to access data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "id": "-R8N0p_mSbg_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB6W9xrfkB9u"
      },
      "source": [
        "#Path variables for the train and test data files\n",
        "train_data = GDRIVE_PROJECT_FOLDER+'train_data_processed1.csv'\n",
        "test_data = GDRIVE_PROJECT_FOLDER+'test_data_processed1.csv'\n",
        "test_true = GDRIVE_PROJECT_FOLDER+'Test_Actual_Final.csv'"
      ],
      "id": "sB6W9xrfkB9u",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Cnx67fgg9hjt",
        "outputId": "260d4b31-25ff-4682-8e5e-9d254d16df6e"
      },
      "source": [
        "#Dataset containing the meme ground truth \n",
        "true_df = pd.read_csv(test_true)\n",
        "true_df.head()"
      ],
      "id": "Cnx67fgg9hjt",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>1_1100_1100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>1_0100_0200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>1_1110_1120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>0_1111_1121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>0_0000_0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                         Image_name       Labels\n",
              "0           0                     chuck_chuck_norris_meme_10.jpg  1_1100_1100\n",
              "1           1                                dr_evil_NDBB96K.png  1_0100_0200\n",
              "2           2      misog_2109e457d636565e2e06dce39874c5231e1.jpg  1_1110_1120\n",
              "3           3   obama_2691536739_469698809820026_263513986_n.jpg  0_1111_1121\n",
              "4           4  kim_threat-kim-jong-un-allegedly-working-on-mu...  0_0000_0000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "RBuzPUmo-B2L",
        "outputId": "e4a252e8-fb3f-4a49-bf3e-9f0481e5796d"
      },
      "source": [
        "#Extracting the first digit (1, 0 , -1) from Labels \n",
        "true_df['Sentiment'] = true_df['Labels'].str.split('_').str[0]\n",
        "true_df['Sentiment'] = true_df['Sentiment'].astype(int)\n",
        "true_df.head()"
      ],
      "id": "RBuzPUmo-B2L",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>1_1100_1100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>1_0100_0200</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>1_1110_1120</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>0_1111_1121</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>0_0000_0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... Sentiment\n",
              "0           0  ...         1\n",
              "1           1  ...         1\n",
              "2           2  ...         1\n",
              "3           3  ...         0\n",
              "4           4  ...         0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "XiyOE5g5TLQg",
        "outputId": "e732f3d1-49d4-49da-bd30-715134748c35"
      },
      "source": [
        "#Dataset containing the Train data\n",
        "train_df = pd.read_csv(train_data, converters={'pre_tokens': eval, 'processed': eval})\n",
        "train_df.head()"
      ],
      "id": "XiyOE5g5TLQg",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>image_name</th>\n",
              "      <th>text_ocr</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall_sentiment</th>\n",
              "      <th>processed</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stop_tokens</th>\n",
              "      <th>rem_punct_tokens</th>\n",
              "      <th>pre_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>image_1.jpg</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[look, friend, lightyear, sohalikut, trend, pl...</td>\n",
              "      <td>['look', 'there', 'my', 'friend', 'lightyear',...</td>\n",
              "      <td>['look', '', '', 'friend', 'lightyear', '', ''...</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "      <td>[look, friend, lightyear, sohalikut, trend, pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>image_2.jpeg</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[best, yearchallenge, complete, less, year, ku...</td>\n",
              "      <td>['the', 'best', 'of', 'yearchallenge', 'comple...</td>\n",
              "      <td>['', 'best', '', 'yearchallenge', 'completed',...</td>\n",
              "      <td>['best', 'yearchallenge', 'completed', 'years'...</td>\n",
              "      <td>[best, yearchalleng, complet, year, kudu, nare...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>image_3.JPG</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[sam, thorne, strippin, follow, follow, saw, e...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>[sam, thorn, strippin, follow, follow, saw, po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>image_4.png</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[year, challenge, sweet, dee, edition]</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>[year, challeng, sweet, dee, edit]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>image_5.png</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[year, challenge, filter, hilarious, year, cha...</td>\n",
              "      <td>['year', 'challenge', 'with', 'no', 'filter', ...</td>\n",
              "      <td>['year', 'challenge', '', '', 'filter', 'hilar...</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "      <td>[year, challeng, filter, hilari, year, challen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         pre_tokens\n",
              "0           0  ...  [look, friend, lightyear, sohalikut, trend, pl...\n",
              "1           1  ...  [best, yearchalleng, complet, year, kudu, nare...\n",
              "2           2  ...  [sam, thorn, strippin, follow, follow, saw, po...\n",
              "3           3  ...                 [year, challeng, sweet, dee, edit]\n",
              "4           4  ...  [year, challeng, filter, hilari, year, challen...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9eea90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccd5c77-e99f-4b9f-ac95-d4c0a89438e0"
      },
      "source": [
        "#Checking the class labels balance in training dataset for task 1 (identifying meme as positive/negative/neutral - (1/-1/0))\n",
        "train_df['overall_sentiment'].value_counts()"
      ],
      "id": "2a9eea90",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    4058\n",
              " 0    2157\n",
              "-1     615\n",
              "Name: overall_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "zMy6wSszkvz4",
        "outputId": "bb3cff04-a15f-40d2-e53d-9e6fe943c2c5"
      },
      "source": [
        "#Dataset containing the processed text of test data\n",
        "test_df = pd.read_csv(test_data, converters={'pre_tokens': eval, 'processed': eval})\n",
        "test_df.head()"
      ],
      "id": "zMy6wSszkvz4",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Image_URL</th>\n",
              "      <th>OCR_extracted_text</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>processed</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stop_tokens</th>\n",
              "      <th>rem_punct_tokens</th>\n",
              "      <th>pre_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>https://gtmemes.com/wp-content/uploads/2019/03...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "      <td>[magician, walk, water, chuck, norris, swim, l...</td>\n",
              "      <td>['some', 'magicians', 'can', 'walk', 'on', 'wa...</td>\n",
              "      <td>['', 'magicians', '', 'walk', '', 'water', 'ch...</td>\n",
              "      <td>['magicians', 'walk', 'water', 'chuck', 'norri...</td>\n",
              "      <td>[magician, walk, water, chuck, norri, swim, land]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>https://i.imgur.com/NDBB96K.png</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "      <td>[one, million, dollar, make, imgur]</td>\n",
              "      <td>['one', 'million', 'dollars', 'made', 'on', 'i...</td>\n",
              "      <td>['', 'million', 'dollars', '', '', 'imgur']</td>\n",
              "      <td>['million', 'dollars', 'imgur']</td>\n",
              "      <td>[million, dollar, imgur]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>https://media0ch-a.akamaihd.net/83/96/9e457d63...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "      <td>[mom, friend, sleep, mom, fine, boy, growingup...</td>\n",
              "      <td>['me', 'mom', 'can', 'my', 'friend', 'sleep', ...</td>\n",
              "      <td>['', 'mom', '', '', 'friend', 'sleep', '', 'mo...</td>\n",
              "      <td>['mom', 'friend', 'sleep', 'mom', 'fine', 'boy...</td>\n",
              "      <td>[mom, friend, sleep, mom, fine, boi, growingup...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>http://politicalmemes.com/wp-content/uploads/2...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "      <td>[guy, inherit, mess, whine, foxed, thing, guy,...</td>\n",
              "      <td>['this', 'guy', 'inherited', 'mess', 'did', 'h...</td>\n",
              "      <td>['', 'guy', 'inherited', 'mess', '', '', 'whin...</td>\n",
              "      <td>['guy', 'inherited', 'mess', 'whine', 'foxed',...</td>\n",
              "      <td>[gui, inherit, mess, whine, fox, thing, gui, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>https://pics.me.me/threat-kim-jong-un-allegedl...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "      <td>[threat, kim, jong, un, allegedly, work, multi...</td>\n",
              "      <td>['threat', 'kim', 'jong', 'un', 'allegedly', '...</td>\n",
              "      <td>['threat', 'kim', 'jong', '', 'allegedly', 'wo...</td>\n",
              "      <td>['threat', 'kim', 'jong', 'allegedly', 'workin...</td>\n",
              "      <td>[threat, kim, jong, allegedli, work, multipl, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         pre_tokens\n",
              "0           0  ...  [magician, walk, water, chuck, norri, swim, land]\n",
              "1           1  ...                           [million, dollar, imgur]\n",
              "2           2  ...  [mom, friend, sleep, mom, fine, boi, growingup...\n",
              "3           3  ...  [gui, inherit, mess, whine, fox, thing, gui, f...\n",
              "4           4  ...  [threat, kim, jong, allegedli, work, multipl, ...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBUfTJn2JZS0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "21262e1f-afe9-49ef-deb8-0b406598fed6"
      },
      "source": [
        "#Creating a duplicate copy of train dataframe, so that modifications can be done in copy df if needed\n",
        "train_df_sub = train_df\n",
        "train_df_sub.head()"
      ],
      "id": "cBUfTJn2JZS0",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>image_name</th>\n",
              "      <th>text_ocr</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall_sentiment</th>\n",
              "      <th>processed</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stop_tokens</th>\n",
              "      <th>rem_punct_tokens</th>\n",
              "      <th>pre_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>image_1.jpg</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[look, friend, lightyear, sohalikut, trend, pl...</td>\n",
              "      <td>['look', 'there', 'my', 'friend', 'lightyear',...</td>\n",
              "      <td>['look', '', '', 'friend', 'lightyear', '', ''...</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "      <td>[look, friend, lightyear, sohalikut, trend, pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>image_2.jpeg</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[best, yearchallenge, complete, less, year, ku...</td>\n",
              "      <td>['the', 'best', 'of', 'yearchallenge', 'comple...</td>\n",
              "      <td>['', 'best', '', 'yearchallenge', 'completed',...</td>\n",
              "      <td>['best', 'yearchallenge', 'completed', 'years'...</td>\n",
              "      <td>[best, yearchalleng, complet, year, kudu, nare...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>image_3.JPG</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[sam, thorne, strippin, follow, follow, saw, e...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>[sam, thorn, strippin, follow, follow, saw, po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>image_4.png</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[year, challenge, sweet, dee, edition]</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>[year, challeng, sweet, dee, edit]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>image_5.png</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[year, challenge, filter, hilarious, year, cha...</td>\n",
              "      <td>['year', 'challenge', 'with', 'no', 'filter', ...</td>\n",
              "      <td>['year', 'challenge', '', '', 'filter', 'hilar...</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "      <td>[year, challeng, filter, hilari, year, challen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         pre_tokens\n",
              "0           0  ...  [look, friend, lightyear, sohalikut, trend, pl...\n",
              "1           1  ...  [best, yearchalleng, complet, year, kudu, nare...\n",
              "2           2  ...  [sam, thorn, strippin, follow, follow, saw, po...\n",
              "3           3  ...                 [year, challeng, sweet, dee, edit]\n",
              "4           4  ...  [year, challeng, filter, hilari, year, challen...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uWmcWQpEzUs"
      },
      "source": [
        "## Multi class classification with CNN"
      ],
      "id": "7uWmcWQpEzUs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y1N_iz-Faoh"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    '''\n",
        "      pretrained_embedding - None or pretrained_embedding\n",
        "                              if none, embedding layer is used create a lookup table to store word embeddings\n",
        "                              else pretrained word embeddings are used in further layers\n",
        "      freeze_embedding - set to true when pretrained embeddings are used to freeze training of word embeddings\n",
        "      vocab_size - size of vocabulary\n",
        "      filter_sizes - set of filters with size (typically represent n-gram)\n",
        "      num_classes - the number of output labels\n",
        "      dropout - for regularization during training phase\n",
        "    '''\n",
        "    def __init__(self, pretrained_embedding, freeze_embedding, vocab_size, embed_dim, filter_sizes, num_filters, num_classes, dropout):\n",
        "        \n",
        "        super(CNN, self).__init__() \n",
        "        \n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding, padding_idx=0)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embed_dim, padding_idx=0, max_norm=5.0)\n",
        "        \n",
        "        # list of 1 dimensional convolutionlayer for each of filters with input size of embedding dimension\n",
        "        # and output size as number of filters of considered filter size\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim, out_channels=num_filters[i], kernel_size=filter_sizes[i])\n",
        "              for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        \n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    '''\n",
        "      word embeddings of a sentence are identified from embedding layer\n",
        "      convolution is performed on the embeddings and features and patterns are identified with the help of kernel\n",
        "      max_pooling is used to identify the max feature from every resultant filter\n",
        "      all the concatenated max features are fed into the linear layer and softmax activation is applied on output\n",
        "    '''\n",
        "    def forward(self, input_ids):\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "        \n",
        "        x = self.fc(self.dropout(x_fc))\n",
        "        x = F.softmax(x, dim = 1)\n",
        "        return x"
      ],
      "id": "_y1N_iz-Faoh",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1vDn4kbNxcF"
      },
      "source": [
        "# Function to get the output tensor\n",
        "def make_target(label):\n",
        "    if label == -1:\n",
        "        return torch.tensor([2], dtype=torch.long, device=device)\n",
        "    elif label == 0:\n",
        "        return torch.tensor([0], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        return torch.tensor([1], dtype=torch.long, device=device)"
      ],
      "id": "i1vDn4kbNxcF",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHrb7vfZqeYm"
      },
      "source": [
        "'''\n",
        "  create_word2idx() creates a vocabulary assigning unique integer to every word in vocabulary\n",
        "'''\n",
        "def create_word2idx(sentences):\n",
        "    max_len = 0\n",
        "    word2idx = {}\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for tokenized_sent in sentences:\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return word2idx, max_len\n",
        "\n",
        "'''\n",
        "  encode() converts a tokenized sentence in list of indices according to word2idx\n",
        "'''\n",
        "def encode(tokenized_sentences, word2idx, max_len):\n",
        "    \n",
        "    input_sentences = []\n",
        "    for tokenized_sent in tokenized_sentences:\n",
        "        #to make all sentences of equal length, remaining length in sentence from max length is padded\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        input_sent = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_sentences.append(input_sent)\n",
        "    \n",
        "    return np.array(input_sentences)"
      ],
      "id": "zHrb7vfZqeYm",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8syi9b6XAk9"
      },
      "source": [
        "Using word2idx and encode methods, the training and test sentences are encoded"
      ],
      "id": "S8syi9b6XAk9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHyEIAnTsY4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6f5a00-3779-4433-c59e-7c07ea1ab1c8"
      },
      "source": [
        "labels_task_1 = ['Neutral', 'Positive', 'Negative']\n",
        "test_df_sub = pd.merge(test_df[['Unnamed: 0','pre_tokens']], true_df[['Unnamed: 0','Sentiment']], on='Unnamed: 0')\n",
        "test_df_sub.rename(columns = {\"Sentiment\": \"sentiment\"}, inplace=True)\n",
        "\n",
        "print(\"Building Word --> indices\")\n",
        "\n",
        "train_sentences_1 = train_df_sub['pre_tokens'].to_list()\n",
        "train_labels_1 = train_df_sub['overall_sentiment'].to_list()\n",
        "train_labels_1 = [2 if label==-1 else label for label in train_labels_1]\n",
        "print(len(train_sentences_1), len(train_labels_1))\n",
        "\n",
        "test_sentences_1 = list(test_df_sub['pre_tokens'])\n",
        "test_labels_1 = test_df_sub['sentiment'].to_list()\n",
        "test_labels_1 = [2 if label==-1 else label for label in test_labels_1]\n",
        "\n",
        "word2idx_1, max_len = create_word2idx(train_sentences_1)\n",
        "\n",
        "print(\"\\nEncoding sentences\")\n",
        "train_input_sentences_1 = encode(train_sentences_1, word2idx_1, max_len)\n",
        "\n",
        "print(train_input_sentences_1)"
      ],
      "id": "GHyEIAnTsY4G",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Word --> indices\n",
            "6830 6830\n",
            "\n",
            "Encoding sentences\n",
            "[[   2    3    4 ...    0    0    0]\n",
            " [  13   14   15 ...    0    0    0]\n",
            " [  22   23   24 ...    0    0    0]\n",
            " ...\n",
            " [ 252  554 9297 ...    0    0    0]\n",
            " [ 168 6542  168 ...    0    0    0]\n",
            " [ 241 1479  177 ...    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O605X3GfXGVh"
      },
      "source": [
        "Using Word2Vec to create pretrained word embedding with window size 5 and embedding dimension as 300"
      ],
      "id": "O605X3GfXGVh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsRcUR2dyHZ5",
        "outputId": "afded92d-7574-4926-f901-29466053160d"
      },
      "source": [
        "sentence_tokens = train_df_sub['pre_tokens']\n",
        "\n",
        "# Initializing the train model\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "print(\"Word2Vec Training model....\")\n",
        "w2v_model = gensim.models.Word2Vec(sentence_tokens, size=300, window=5, min_count=2, sg = 1,\n",
        "                                      hs = 0, negative = 10, workers= 32, seed = 34) \n"
      ],
      "id": "UsRcUR2dyHZ5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec Training model....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23xqJ5HdVlP7"
      },
      "source": [
        "def create_embedding_vectors(embedding, vocab):\n",
        "  vocab_size = len(vocab) + 1\n",
        "  embedding_vectors = np.zeros((vocab_size, 300))\n",
        "\n",
        "  for word, i in vocab.items():\n",
        "    if word in embedding.wv.vocab:\n",
        "      embedding_vectors[i] = embedding.wv[word]\n",
        "    else:\n",
        "      embedding_vectors[i] = np.zeros(300)\n",
        "  return embedding_vectors\n",
        "\n",
        "embedding_vectors = create_embedding_vectors(w2v_model, word2idx_1)"
      ],
      "id": "23xqJ5HdVlP7",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978SdQvHXUIC"
      },
      "source": [
        "Dataloader method is used to prepare batch dataset for training and validation.\n",
        "Each batch is of size 32 i.e 32 instances in each batch"
      ],
      "id": "978SdQvHXUIC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za3FO3J2_AOL"
      },
      "source": [
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50):\n",
        "    \n",
        "    # Convert data type to torch.Tensor\n",
        "    train_inputs, val_inputs, train_labels, val_labels = tuple(torch.tensor(data) for data in [train_inputs, val_inputs, train_labels, val_labels])\n",
        "\n",
        "    # Create DataLoader for training data\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Create DataLoader for validation data\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ],
      "id": "Za3FO3J2_AOL",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FVmc61VZnar"
      },
      "source": [
        "# preparing batch dataloader\n",
        "train_input_sentences_1, val_input_sentences_1, train_labels_1, val_labels_1 = train_test_split(train_input_sentences_1, train_labels_1, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataloader_1, val_dataloader_1 = data_loader(train_input_sentences_1, val_input_sentences_1, train_labels_1, val_labels_1, batch_size=32)"
      ],
      "id": "5FVmc61VZnar",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BChpgfqBYPkD"
      },
      "source": [
        "Object of CNN class is initialized with \n",
        "\n",
        "\n",
        "1.   pretrained_embedding to None - Using embedding layer for word embeddings\n",
        "2.   freeze_embedding to false - since pretrained_embedding is not used\n",
        "3.   vocab_size to size of word2idx vocabulary\n",
        "4.   embed_dim to 100 - each word embedding dimension\n",
        "5.   filter_sizes - size of kernels\n",
        "6.   num_classes - 3 (positive, negative, neutral)\n",
        "7.   dropout - regularization to 0.5\n",
        "\n",
        "Adadelta optimizer is used with learning rate 0.25 and Cross entropy loss is used for this multi class classification\n",
        "\n"
      ],
      "id": "BChpgfqBYPkD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1gwKReBeTz9"
      },
      "source": [
        "#pretrained_embedding = torch.FloatTensor(embedding_vectors)\n",
        "pretrained_embedding = None\n",
        "epochs = 10\n",
        " \n",
        "cnn_model_1 = CNN(pretrained_embedding=pretrained_embedding,\n",
        "                        freeze_embedding=False,\n",
        "                        vocab_size=len(word2idx_1),\n",
        "                        embed_dim=100,\n",
        "                        filter_sizes=[2, 4, 6], # bigram, 4-gram, 6-gram\n",
        "                        num_filters=[100, 100, 100],\n",
        "                        num_classes=3,\n",
        "                        dropout=0.5)\n",
        "    \n",
        "cnn_model_1.to(device)\n",
        "\n",
        "optimizer_1 = optim.Adadelta(cnn_model_1.parameters(), lr=0.25, rho=0.95)\n",
        "loss_fn_1 = nn.CrossEntropyLoss()\n",
        "#loss_fn_1 = nn.BCELoss()"
      ],
      "id": "f1gwKReBeTz9",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-28gg5XwaO5t",
        "outputId": "e6d723f2-2bfc-49b2-f608-c909191e13bf"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  cnn_model_1.train()\n",
        "\n",
        "  train_loss = 0\n",
        "  train_accuracy = []\n",
        "\n",
        "  #for every batch\n",
        "  for i, batch in enumerate(train_dataloader_1):\n",
        "      sentences, y_original = tuple(sent.to(device) for sent in batch)\n",
        "      cnn_model_1.zero_grad()\n",
        "\n",
        "      # feed forward sentence to get the class probabilities\n",
        "      y_predicted = cnn_model_1(sentences)\n",
        "\n",
        "      #y_original = y_original.float()\n",
        "\n",
        "      # loss function is calculated with original labels against the predicted labes\n",
        "      loss = loss_fn_1(y_predicted, y_original)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # to find accuracy in each batch\n",
        "      y_pred_label = torch.argmax(y_predicted, dim=1).flatten()\n",
        "      correct = 0\n",
        "      for i in range(len(y_original)):\n",
        "        if(y_original[i] == y_pred_label[i]):\n",
        "          correct +=1\n",
        "\n",
        "      train_accuracy.append(correct/len(y_original))\n",
        "\n",
        "      # backpropagation of loss\n",
        "      loss.backward()\n",
        "      optimizer_1.step()\n",
        "\n",
        "  print(\"\\nTraining: Epoch {} --> Loss {} | Accuracy {}\".format(epoch, train_loss/len(train_dataloader_1), np.mean(train_accuracy)))\n",
        "\n",
        "  # validation is carried out on the validation dataset\n",
        "  if val_dataloader_1 is not None:\n",
        "\n",
        "    cnn_model_1.eval()\n",
        "    val_accuracy = []\n",
        "    f1_val_nn = []\n",
        "\n",
        "    for batch in val_dataloader_1:\n",
        "        sentences, y_original = tuple(sent.to(device) for sent in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_predicted = cnn_model_1(sentences)\n",
        "\n",
        "        y_pred_label = torch.argmax(y_predicted, dim=1).flatten()\n",
        "\n",
        "        correct = 0\n",
        "        for i in range(len(y_original)):\n",
        "          if(y_original[i] == y_pred_label[i]):\n",
        "            correct +=1\n",
        "\n",
        "        val_accuracy.append(correct/len(y_original))\n",
        "        f1_val_nn.append(f1_score(y_original, y_pred_label, average = 'macro'))\n",
        "\n",
        "    print(\"Validation: Epoch {} --> Accuracy {} | F1 Score: {}\".format(epoch, np.mean(val_accuracy), np.mean(f1_val_nn)))"
      ],
      "id": "-28gg5XwaO5t",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training: Epoch 0 --> Loss 0.9717246374318019 | Accuracy 0.5874352331606217\n",
            "Validation: Epoch 0 --> Accuracy 0.6140237603305785 | F1 Score: 0.25845184498429224\n",
            "\n",
            "Training: Epoch 1 --> Loss 0.9557980459588797 | Accuracy 0.5942357512953368\n",
            "Validation: Epoch 1 --> Accuracy 0.6140237603305785 | F1 Score: 0.25845184498429224\n",
            "\n",
            "Training: Epoch 2 --> Loss 0.9508428826850931 | Accuracy 0.5933182210708117\n",
            "Validation: Epoch 2 --> Accuracy 0.6140237603305785 | F1 Score: 0.25845184498429224\n",
            "\n",
            "Training: Epoch 3 --> Loss 0.9436038760323598 | Accuracy 0.5925626079447324\n",
            "Validation: Epoch 3 --> Accuracy 0.6055010330578512 | F1 Score: 0.2623534682183835\n",
            "\n",
            "Training: Epoch 4 --> Loss 0.9302946372353351 | Accuracy 0.6094559585492227\n",
            "Validation: Epoch 4 --> Accuracy 0.6126033057851239 | F1 Score: 0.2580560503027832\n",
            "\n",
            "Training: Epoch 5 --> Loss 0.9146329260243036 | Accuracy 0.6285082037996547\n",
            "Validation: Epoch 5 --> Accuracy 0.5854855371900827 | F1 Score: 0.2940398850603484\n",
            "\n",
            "Training: Epoch 6 --> Loss 0.8962883819570195 | Accuracy 0.6548467184801381\n",
            "Validation: Epoch 6 --> Accuracy 0.5769628099173554 | F1 Score: 0.2936368703490413\n",
            "\n",
            "Training: Epoch 7 --> Loss 0.8765259947183837 | Accuracy 0.6781088082901554\n",
            "Validation: Epoch 7 --> Accuracy 0.596849173553719 | F1 Score: 0.28501558445073943\n",
            "\n",
            "Training: Epoch 8 --> Loss 0.8601147862913695 | Accuracy 0.7002914507772021\n",
            "Validation: Epoch 8 --> Accuracy 0.6011105371900827 | F1 Score: 0.288071271086524\n",
            "\n",
            "Training: Epoch 9 --> Loss 0.8446580125260229 | Accuracy 0.7162132987910191\n",
            "Validation: Epoch 9 --> Accuracy 0.5870351239669421 | F1 Score: 0.29627921148194164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSCHQdrNYxyt"
      },
      "source": [
        "# predict() takes the sentence and label as parameters, use the model to predict the label of the sentence\n",
        "# and returns 1 if it predicts correctly else 0\n",
        "\n",
        "def predict_1(text, label):\n",
        "    max_len = 100\n",
        "\n",
        "    # Tokenize, pad and encode text\n",
        "    padded_tokens = text + ['<pad>'] * (max_len - len(text))\n",
        "    input_id = [word2idx_1.get(token) if word2idx_1.get(token)!=None else 1 for token in padded_tokens]\n",
        "\n",
        "    test_sentence = torch.tensor(input_id).unsqueeze(dim=0)\n",
        "\n",
        "    y_predicted = cnn_model_1.forward(test_sentence)\n",
        "    preds = torch.argmax(y_predicted, dim=1).flatten()\n",
        "\n",
        "    if preds == label:\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "\n",
        "    return pred"
      ],
      "id": "PSCHQdrNYxyt",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD_J4QhsbPHI"
      },
      "source": [
        "For each of the sentence in test dataset, prediction is carried out and performance measures are displayed"
      ],
      "id": "sD_J4QhsbPHI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVTh5naR9lwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f0f28e-9a3c-462e-da5e-2b99cd7b8d5a"
      },
      "source": [
        "test_accuracy = 0\n",
        "\n",
        "correct_one = 0\n",
        "total_one = 0\n",
        "correct_two = 0\n",
        "total_two = 0\n",
        "correct_zero= 0\n",
        "total_zero = 0\n",
        "predicted_labels = []\n",
        "for i in range(len(test_sentences_1)):\n",
        "  pred = predict_1(test_sentences_1[i], test_labels_1[i])\n",
        "  predicted_labels.append(pred)\n",
        "  test_accuracy += pred\n",
        "\n",
        "  if test_labels_1[i] == 0:\n",
        "    total_zero += 1\n",
        "    if pred == 1:\n",
        "      correct_zero += 1\n",
        "\n",
        "  if test_labels_1[i] == 1:\n",
        "    total_one += 1\n",
        "    if pred == 1:\n",
        "      correct_one += 1\n",
        "  \n",
        "  if test_labels_1[i] == 2:\n",
        "    total_two += 1\n",
        "    if pred == 1:\n",
        "      correct_two += 1\n",
        "\n",
        "f1 = f1_score(test_labels_1, predicted_labels, average='macro')\n",
        "print(\"\\nTotal Test sentences: {}\".format(len(test_sentences_1)))\n",
        "print(\"\\nTotal neutral test sentences: {}\".format(total_zero))\n",
        "print(\"Nuetral test sentences predicted correctly: {}\".format(correct_zero))\n",
        "\n",
        "print(\"\\nTotal positive test sentences: {}\".format(total_one))\n",
        "print(\"Positive test sentences predicted correctly: {}\".format(correct_one))\n",
        "\n",
        "print(\"\\nTotal negative test sentences: {}\".format(total_two))\n",
        "print(\"Negative test sentences predicted correctly: {}\".format(correct_two))\n",
        "print(\"\\nTest Accuracy: {}\".format(test_accuracy/len(test_sentences_1)))\n",
        "print(\"Test F1 Score: {} \\n\".format(f1))\n",
        "\n",
        "print(classification_report(test_labels_1, predicted_labels, target_names = labels_task_1))"
      ],
      "id": "MVTh5naR9lwt",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total Test sentences: 1840\n",
            "\n",
            "Total neutral test sentences: 580\n",
            "Nuetral test sentences predicted correctly: 26\n",
            "\n",
            "Total positive test sentences: 1089\n",
            "Positive test sentences predicted correctly: 1019\n",
            "\n",
            "Total negative test sentences: 171\n",
            "Negative test sentences predicted correctly: 0\n",
            "\n",
            "Test Accuracy: 0.5679347826086957\n",
            "Test F1 Score: 0.5869440799750079 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Neutral       0.70      0.96      0.81       580\n",
            "    Positive       0.98      0.94      0.96      1089\n",
            "    Negative       0.00      0.00      0.00       171\n",
            "\n",
            "    accuracy                           0.85      1840\n",
            "   macro avg       0.56      0.63      0.59      1840\n",
            "weighted avg       0.80      0.85      0.82      1840\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbt6Ed5iaIGq"
      },
      "source": [
        "# Multi Label classification with CNN"
      ],
      "id": "Sbt6Ed5iaIGq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7rwy3kPaOfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97329210-2171-41d1-df83-a43a63b683c2"
      },
      "source": [
        "train_data = '/content/gdrive/MyDrive/NLP_Project/train_data_processed1.csv'\n",
        "train_df=pd.read_csv(train_data)\n",
        "train_df.info()           # explore data frame information "
      ],
      "id": "W7rwy3kPaOfC",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6830 entries, 0 to 6829\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Unnamed: 0         6830 non-null   int64 \n",
            " 1   Unnamed: 0.1       6830 non-null   int64 \n",
            " 2   image_name         6830 non-null   object\n",
            " 3   text_ocr           6830 non-null   object\n",
            " 4   text_corrected     6830 non-null   object\n",
            " 5   humour             6830 non-null   int64 \n",
            " 6   sarcasm            6830 non-null   int64 \n",
            " 7   offensive          6830 non-null   int64 \n",
            " 8   motivational       6830 non-null   int64 \n",
            " 9   overall_sentiment  6830 non-null   int64 \n",
            " 10  processed          6830 non-null   object\n",
            " 11  tokenized_text     6830 non-null   object\n",
            " 12  stop_tokens        6830 non-null   object\n",
            " 13  rem_punct_tokens   6830 non-null   object\n",
            " 14  pre_tokens         6830 non-null   object\n",
            "dtypes: int64(7), object(8)\n",
            "memory usage: 800.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPUhnoDwal33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "41beb698-fdf5-4c08-969f-641dc0054a9d"
      },
      "source": [
        "# dropping unnecessary columns\n",
        "train_df = train_df.drop(['Unnamed: 0','image_name','text_ocr','overall_sentiment'], axis=1)  # Drop some features\n",
        "train_df.head()"
      ],
      "id": "DPUhnoDwal33",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>processed</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>stop_tokens</th>\n",
              "      <th>rem_punct_tokens</th>\n",
              "      <th>pre_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "      <td>['look', 'there', 'my', 'friend', 'lightyear',...</td>\n",
              "      <td>['look', '', '', 'friend', 'lightyear', '', ''...</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['best', 'yearchallenge', 'complete', 'less', ...</td>\n",
              "      <td>['the', 'best', 'of', 'yearchallenge', 'comple...</td>\n",
              "      <td>['', 'best', '', 'yearchallenge', 'completed',...</td>\n",
              "      <td>['best', 'yearchallenge', 'completed', 'years'...</td>\n",
              "      <td>['best', 'yearchalleng', 'complet', 'year', 'k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "      <td>['sam', 'thorn', 'strippin', 'follow', 'follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "      <td>['year', 'challeng', 'sweet', 'dee', 'edit']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "      <td>['year', 'challenge', 'with', 'no', 'filter', ...</td>\n",
              "      <td>['year', 'challenge', '', '', 'filter', 'hilar...</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "      <td>['year', 'challeng', 'filter', 'hilari', 'year...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0.1  ...                                         pre_tokens\n",
              "0             0  ...  ['look', 'friend', 'lightyear', 'sohalikut', '...\n",
              "1             1  ...  ['best', 'yearchalleng', 'complet', 'year', 'k...\n",
              "2             2  ...  ['sam', 'thorn', 'strippin', 'follow', 'follow...\n",
              "3             3  ...       ['year', 'challeng', 'sweet', 'dee', 'edit']\n",
              "4             4  ...  ['year', 'challeng', 'filter', 'hilari', 'year...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6tK0gXOak9y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3f3dd70a-d73c-4539-b3f4-8b0151a5fe62"
      },
      "source": [
        "'''train_df.humour[train_df['humour']!='not_funny']=1\n",
        "train_df.humour[train_df['humour']=='not_funny']=0\n",
        "train_df.sarcasm[train_df['sarcasm']!='not_sarcastic']=1\n",
        "train_df.sarcasm[train_df['sarcasm']=='not_sarcastic']=0\n",
        "train_df.offensive[train_df['offensive']!='not_offensive']=1\n",
        "train_df.offensive[train_df['offensive']=='not_offensive']=0\n",
        "train_df.motivational[train_df['motivational']!='not_motivational']=1\n",
        "train_df.motivational[train_df['motivational']=='not_motivational']=0\n",
        "\n",
        "train_df'''"
      ],
      "id": "X6tK0gXOak9y",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"train_df.humour[train_df['humour']!='not_funny']=1\\ntrain_df.humour[train_df['humour']=='not_funny']=0\\ntrain_df.sarcasm[train_df['sarcasm']!='not_sarcastic']=1\\ntrain_df.sarcasm[train_df['sarcasm']=='not_sarcastic']=0\\ntrain_df.offensive[train_df['offensive']!='not_offensive']=1\\ntrain_df.offensive[train_df['offensive']=='not_offensive']=0\\ntrain_df.motivational[train_df['motivational']!='not_motivational']=1\\ntrain_df.motivational[train_df['motivational']=='not_motivational']=0\\n\\ntrain_df\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAovzcv-uZqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88bc387-1772-4170-e2d1-c8dcb202727d"
      },
      "source": [
        "print(train_df['humour'].value_counts())\n",
        "print(train_df['sarcasm'].value_counts())\n",
        "print(train_df['offensive'].value_counts())\n",
        "print(train_df['motivational'].value_counts())"
      ],
      "id": "oAovzcv-uZqG",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    5212\n",
            "0    1618\n",
            "Name: humour, dtype: int64\n",
            "1    5314\n",
            "0    1516\n",
            "Name: sarcasm, dtype: int64\n",
            "1    4173\n",
            "0    2657\n",
            "Name: offensive, dtype: int64\n",
            "0    4421\n",
            "1    2409\n",
            "Name: motivational, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAqzu71Ugku0"
      },
      "source": [
        "class CNN_2(nn.Module):\n",
        "    '''\n",
        "      pretrained_embedding - None or pretrained_embedding\n",
        "                              if none, embedding layer is used create a lookup table to store word embeddings\n",
        "                              else pretrained word embeddings are used in further layers\n",
        "      freeze_embedding - set to true when pretrained embeddings are used to freeze training of word embeddings\n",
        "      vocab_size - size of vocabulary\n",
        "      filter_sizes - set of filters with size (typically represent n-gram)\n",
        "      num_classes - the number of output labels\n",
        "      dropout - for regularization during training phase\n",
        "    '''\n",
        "    def __init__(self, pretrained_embedding, freeze_embedding, vocab_size, embed_dim, filter_sizes, num_filters, num_classes, dropout):\n",
        "        \n",
        "        super(CNN_2, self).__init__()\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding, freeze=freeze_embedding, padding_idx=0)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embed_dim, padding_idx=0, max_norm=5.0)\n",
        "        \n",
        "        # list of 1 dimensional convolutionlayer for each of filters with input size of embedding dimension\n",
        "        # and output size as number of filters of considered filter size\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "              nn.Conv1d(in_channels=self.embed_dim,\n",
        "                        out_channels=num_filters[i],\n",
        "                        kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    '''\n",
        "      word embeddings of a sentence are identified from embedding layer\n",
        "      convolution is performed on the embeddings and features and patterns are identified with the help of kernel\n",
        "      max_pooling is used to identify the max feature from every resultant filter\n",
        "      all the concatenated max features are fed into the linear layer and sigmoid activation is applied on output\n",
        "    '''\n",
        "    def forward(self, input_ids):\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        \n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
        "        x = self.fc(self.dropout(x_fc))\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "id": "YAqzu71Ugku0",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvrtRbsQ8kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0ccbca-e8ef-4c81-f7fb-c10692d9138e"
      },
      "source": [
        "labels_task_2 = ['Humour', 'Sarcasm', 'Offensive', 'Motivational']\n",
        "test_df_sub = pd.merge(test_df[['Unnamed: 0','pre_tokens']], true_df[['Unnamed: 0','Labels']], on='Unnamed: 0')\n",
        "\n",
        "test_df_sub['Individual Labels'] = test_df_sub['Labels'].apply(lambda label: label.split('_')[1])\n",
        "test_df_sub = test_df_sub.drop(columns = ['Unnamed: 0', 'Labels'])\n",
        "\n",
        "separate_labels = test_df_sub['Individual Labels'].apply(lambda x: pd.Series(list(x)))\n",
        "\n",
        "labels = ['humour', 'sarcasm', 'offensive', 'motivational']\n",
        "\n",
        "for i in range(4):\n",
        "  test_df_sub[labels[i]] = separate_labels[i]\n",
        "\n",
        "test_sentences_2 = test_df_sub['pre_tokens'].to_list()\n",
        "test_labels_2 = test_df_sub[['humour', 'sarcasm', 'offensive', 'motivational']].to_numpy()\n",
        "print(len(test_sentences_2), len(test_labels_2))"
      ],
      "id": "jGvrtRbsQ8kB",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1840 1840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiI6VYSidWzr"
      },
      "source": [
        "Using word2idx and encode methods, the training and test sentences are encoded"
      ],
      "id": "OiI6VYSidWzr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D9F9X3hi39E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f84a1d-7d45-4fe1-bae2-614ae3d8f2e1"
      },
      "source": [
        "print(\"Building Word --> indices\")\n",
        "\n",
        "train_sentences_2 = train_df_sub['pre_tokens'].to_list()\n",
        "train_labels_2 = train_df[['humour', 'sarcasm', 'offensive', 'motivational']].to_numpy()\n",
        "print(len(train_sentences_2), len(train_labels_2))\n",
        "\n",
        "word2idx_2, max_len = create_word2idx(train_sentences_2)\n",
        "\n",
        "print(\"\\nEncoding sentences\")\n",
        "train_input_sentences_2 = encode(train_sentences_2, word2idx_2, max_len)\n",
        "\n",
        "print(train_input_sentences_2)\n",
        "\n",
        "print(train_labels_2)"
      ],
      "id": "2D9F9X3hi39E",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Word --> indices\n",
            "6830 6830\n",
            "\n",
            "Encoding sentences\n",
            "[[   2    3    4 ...    0    0    0]\n",
            " [  13   14   15 ...    0    0    0]\n",
            " [  22   23   24 ...    0    0    0]\n",
            " ...\n",
            " [ 252  554 9297 ...    0    0    0]\n",
            " [ 168 6542  168 ...    0    0    0]\n",
            " [ 241 1479  177 ...    0    0    0]]\n",
            "[[1 1 0 0]\n",
            " [0 1 0 1]\n",
            " [1 0 0 0]\n",
            " ...\n",
            " [1 1 1 0]\n",
            " [0 1 0 1]\n",
            " [0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8W0cfzVdfIv"
      },
      "source": [
        "Object of CNN_2 class is initialized with \n",
        "\n",
        "\n",
        "1.   pretrained_embedding to None - Using embedding layer for word embeddings\n",
        "2.   freeze_embedding to false - since pretrained_embedding is not used\n",
        "3.   vocab_size to size of word2idx vocabulary\n",
        "4.   embed_dim to 100 - each word embedding dimension\n",
        "5.   filter_sizes - size of kernels\n",
        "6.   num_classes - 4 (humour, sarcasm, offensive and motivational)\n",
        "7.   dropout - regularization to 0.5\n",
        "\n",
        "Adam optimizer is used with learning rate 0.01 and Binary Cross entropy loss is used for this multi label classification"
      ],
      "id": "s8W0cfzVdfIv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns4GyYI3hWyT"
      },
      "source": [
        "#pretrained_embedding = torch.FloatTensor(embedding_vectors)\n",
        "pretrained_embedding = None\n",
        "epochs = 10\n",
        "\n",
        "cnn_model_2 = CNN_2(pretrained_embedding=pretrained_embedding,\n",
        "                        freeze_embedding=False,\n",
        "                        vocab_size=len(word2idx_2),\n",
        "                        embed_dim=100,\n",
        "                        filter_sizes=[2, 4, 6],\n",
        "                        num_filters=[50, 50, 50],\n",
        "                        num_classes=4,\n",
        "                        dropout=0.5)\n",
        "    \n",
        "cnn_model_2.to(device)\n",
        "\n",
        "optimizer_2 = optim.Adam(cnn_model_2.parameters(), lr=0.01)\n",
        "loss_fn_2 = nn.BCELoss()"
      ],
      "id": "ns4GyYI3hWyT",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZOrtdLRd50R"
      },
      "source": [
        "Training labels are converted to tensors and then\n",
        "Dataloader method is used to prepare batch dataset for training and validation.\n",
        "Each batch is of size 32 i.e 32 instances in each batch"
      ],
      "id": "XZOrtdLRd50R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1nhSfegn7AV"
      },
      "source": [
        "train_labels_tensor_2 = []\n",
        "for labels in train_labels_2:\n",
        "  labels_tensor = []\n",
        "  for label in labels:\n",
        "    label_tensor = torch.tensor(label)\n",
        "    labels_tensor.append(label_tensor)\n",
        "  train_labels_tensor_2.append(labels_tensor)\n",
        "\n",
        "train_input_sentences_2, val_input_sentences_2, train_labels_2, val_labels_2 = train_test_split(train_input_sentences_2, train_labels_tensor_2, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataloader_2, val_dataloader_2 = data_loader(train_input_sentences_2, val_input_sentences_2, train_labels_2, val_labels_2, batch_size=32)"
      ],
      "id": "Y1nhSfegn7AV",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlWoyFshcQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c57394b-9c22-4340-d577-0190a6d28597"
      },
      "source": [
        "print(\"\\nValidation accuracy here is displayed as\")\n",
        "print(\"only the total number of instances where the model predicts all the labels correctly\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  cnn_model_2.train()\n",
        "\n",
        "  train_loss = 0\n",
        "  validation_loss = 0\n",
        "  train_accuracy = []\n",
        "\n",
        "  # for every batch\n",
        "  for i, batch in enumerate(train_dataloader_2):\n",
        "      sentences, y_original = tuple(sent.to(device) for sent in batch)\n",
        "      y_original = y_original.float()\n",
        "      cnn_model_2.zero_grad()\n",
        "\n",
        "      # feed forward sentence to calculate probabilities of all labels\n",
        "      y_predicted = cnn_model_2(sentences)\n",
        "\n",
        "      # loss function is calculated with original labels against the predicted labes\n",
        "      loss = loss_fn_2(y_predicted, y_original)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      correct = 0\n",
        "      for i in range(len(y_original)):\n",
        "        y_correct = torch.count_nonzero(y_original[i] == y_predicted[i])\n",
        "        y_correct_int = torch.IntTensor.item(y_correct)\n",
        "        if y_correct_int == 4:\n",
        "          correct +=1\n",
        "\n",
        "      train_accuracy.append(correct/len(y_original))\n",
        "\n",
        "      # loss is backpropagated\n",
        "      loss.backward()\n",
        "      optimizer_2.step()\n",
        "\n",
        "  print(\"\\nTraining: Epoch {} --> Loss {}\".format(epoch, train_loss/len(train_dataloader_2)))\n",
        "\n",
        "  # validation is carried out on validation dataset\n",
        "  if val_dataloader_2 is not None:\n",
        "\n",
        "    cnn_model_2.eval()\n",
        "    val_accuracy = []\n",
        "    f1_val_nn2 = []\n",
        "\n",
        "    for batch in val_dataloader_2:\n",
        "        sentences, y_original = tuple(sent.to(device) for sent in batch)\n",
        "        y_original = y_original.float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_predicted = cnn_model_2(sentences)\n",
        "\n",
        "        for i in range(len(y_predicted)):\n",
        "          y_predicted[i] = (y_predicted[i]>0.5).float()\n",
        "        validation_loss += loss_fn_2(y_predicted, y_original).item()\n",
        "\n",
        "        correct = 0\n",
        "        for i in range(len(y_original)):\n",
        "          y_correct = torch.count_nonzero(y_original[i] == y_predicted[i])\n",
        "          y_correct_int = torch.IntTensor.item(y_correct)\n",
        "          if y_correct_int == 4:\n",
        "            correct +=1\n",
        "\n",
        "        val_accuracy.append(correct/len(y_original))\n",
        "        f1_val_nn2.append(f1_score(y_original, y_predicted, average = 'macro'))\n",
        "\n",
        "    print(\"Validation: Epoch {} --> Accuracy {} | F1 Score: {}\".format(epoch, np.mean(val_accuracy), np.mean(f1_val_nn2)))\n",
        "    #print(\"\\nTraining: Epoch {} --> Loss {}\".format(epoch, validation_loss/len(val_dataloader_2)))"
      ],
      "id": "mVlWoyFshcQR",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation accuracy here is displayed as\n",
            "only the total number of instances where the model predicts all the labels correctly\n",
            "\n",
            "Training: Epoch 0 --> Loss 0.6557005822967371\n",
            "Validation: Epoch 0 --> Accuracy 0.2168130165289256 | F1 Score: 0.6209893646365395\n",
            "\n",
            "Training: Epoch 1 --> Loss 0.6240985461467289\n",
            "Validation: Epoch 1 --> Accuracy 0.2055785123966942 | F1 Score: 0.6100827070425178\n",
            "\n",
            "Training: Epoch 2 --> Loss 0.5388907726873388\n",
            "Validation: Epoch 2 --> Accuracy 0.1958935950413223 | F1 Score: 0.6280365951475714\n",
            "\n",
            "Training: Epoch 3 --> Loss 0.4394507664472946\n",
            "Validation: Epoch 3 --> Accuracy 0.18879132231404958 | F1 Score: 0.6532169939918597\n",
            "\n",
            "Training: Epoch 4 --> Loss 0.35521297774475474\n",
            "Validation: Epoch 4 --> Accuracy 0.15896177685950413 | F1 Score: 0.5983081012314552\n",
            "\n",
            "Training: Epoch 5 --> Loss 0.31134707901453107\n",
            "Validation: Epoch 5 --> Accuracy 0.1518595041322314 | F1 Score: 0.6303541440682076\n",
            "\n",
            "Training: Epoch 6 --> Loss 0.2834907699897499\n",
            "Validation: Epoch 6 --> Accuracy 0.16761363636363635 | F1 Score: 0.6023033222322617\n",
            "\n",
            "Training: Epoch 7 --> Loss 0.2672878814175957\n",
            "Validation: Epoch 7 --> Accuracy 0.17897727272727273 | F1 Score: 0.6223568638871705\n",
            "\n",
            "Training: Epoch 8 --> Loss 0.24719843534762379\n",
            "Validation: Epoch 8 --> Accuracy 0.17174586776859505 | F1 Score: 0.6187408179837308\n",
            "\n",
            "Training: Epoch 9 --> Loss 0.274219108986731\n",
            "Validation: Epoch 9 --> Accuracy 0.15767045454545456 | F1 Score: 0.6375038333501638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU8P2L61bH07"
      },
      "source": [
        "'''\n",
        "  predict() takes the sentence and label as parameters, use the model to predict the probabilites of each label\n",
        "  since sigmoid function converts the value between 0 and 1, if the predicted probability is greater than 0.5,\n",
        "  the label is associated to the sentence else not\n",
        "'''\n",
        "def predict_2(text, label):\n",
        "\n",
        "    max_len = 100\n",
        "    padded_tokens = text + ['<pad>'] * (max_len - len(text))\n",
        "    sentence = [word2idx_2.get(token) if word2idx_2.get(token)!=None else 1 for token in padded_tokens]\n",
        "\n",
        "    sentence = torch.tensor(sentence).unsqueeze(dim=0)\n",
        "\n",
        "    y_predicted = cnn_model_2.forward(sentence)\n",
        "    y_predicted = (y_predicted>0.5).float()\n",
        "    \n",
        "    label = torch.tensor(label)\n",
        "    y_correct = torch.count_nonzero(label == y_predicted)\n",
        "    y_correct_bool = label == y_predicted\n",
        "    y_correct_int = torch.IntTensor.item(y_correct)\n",
        "\n",
        "    return y_predicted, y_correct_bool, y_correct, y_correct_int"
      ],
      "id": "IU8P2L61bH07",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12VFLUcLfOxZ"
      },
      "source": [
        "Prediction is carried out for all the test sentences and the performance measures are displayed"
      ],
      "id": "12VFLUcLfOxZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOZrnQGmNLeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd177ce2-472c-4a04-9b2f-ef386cbccb88"
      },
      "source": [
        "test_full_accuracy = 0\n",
        "one_correct = 0\n",
        "two_correct = 0\n",
        "three_correct = 0\n",
        "test_full_incorrect = 0\n",
        "humour = 0\n",
        "sarcasm = 0\n",
        "offensive = 0\n",
        "motivational = 0\n",
        "predicted_labels_2 = []\n",
        "test_labels_int_2 = []\n",
        "\n",
        "for i in range(len(test_sentences_2)):\n",
        "  test_labels_int_2.append(list(map(int, test_labels_2[i])))\n",
        "  test_labels_tensor = []\n",
        "  for label in test_labels_2[i]:\n",
        "    test_labels_tensor.append(torch.tensor(int(label)))\n",
        "  y_predicted, correct_bool, correct_tensor, correct_int = predict_2(test_sentences_2[i], test_labels_tensor)\n",
        "\n",
        "  predicted_labels_2.append(y_predicted[0])\n",
        "  if correct_int == 4:\n",
        "    test_full_accuracy += 1\n",
        "  elif correct_int == 3:\n",
        "    three_correct += 1\n",
        "  elif correct_int == 2:\n",
        "    two_correct += 1\n",
        "  elif correct_int == 1:\n",
        "    one_correct += 1\n",
        "  else:\n",
        "    test_full_incorrect += 1\n",
        "\n",
        "  if correct_bool[0][0]:\n",
        "    humour += 1\n",
        "  if correct_bool[0][1]:\n",
        "    sarcasm += 1\n",
        "  if correct_bool[0][2]:\n",
        "    offensive += 1\n",
        "  if correct_bool[0][3]:\n",
        "    motivational += 1\n",
        "\n",
        "pred_labels_2 = []\n",
        "for pred in predicted_labels_2:\n",
        "  pred_labels_2.append(pred.cpu().detach().numpy().tolist())\n",
        "\n",
        "f1_2 = f1_score(test_labels_int_2, pred_labels_2, average='macro')\n",
        "print(\"\\nTotal test instances: {}\".format(len(test_sentences_2)))\n",
        "print(\"All correct: {}\".format(test_full_accuracy))\n",
        "print(\"Utmost 3 correct: {}\".format(three_correct))\n",
        "print(\"Utmost 2 correct: {}\".format(two_correct))\n",
        "print(\"Only 1 correct: {}\".format(one_correct))\n",
        "print(\"All incorrect: {}\".format(test_full_incorrect))\n",
        "\n",
        "print(\"\\nAccuracies Label Wise\")\n",
        "print(\"humour: {}\".format(humour/len(test_sentences_2)))\n",
        "print(\"Sarcasm: {}\".format(sarcasm/len(test_sentences_2)))\n",
        "print(\"Offensive: {}\".format(offensive/len(test_sentences_2)))\n",
        "print(\"Motivational: {} \\n\".format(motivational/len(test_sentences_2)))\n",
        "print(classification_report(test_labels_int_2, pred_labels_2, target_names=labels_task_2))\n",
        "print(\"\\nF1 Score with macro average: {}\".format(f1_2))"
      ],
      "id": "KOZrnQGmNLeN",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total test instances: 1840\n",
            "All correct: 287\n",
            "Utmost 3 correct: 646\n",
            "Utmost 2 correct: 552\n",
            "Only 1 correct: 281\n",
            "All incorrect: 74\n",
            "\n",
            "Accuracies Label Wise\n",
            "humour: 0.6657608695652174\n",
            "Sarcasm: 0.7146739130434783\n",
            "Offensive: 0.5451086956521739\n",
            "Motivational: 0.5043478260869565 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Humour       0.76      0.82      0.79      1402\n",
            "     Sarcasm       0.77      0.89      0.83      1424\n",
            "   Offensive       0.61      0.74      0.67      1146\n",
            "Motivational       0.37      0.50      0.42       678\n",
            "\n",
            "   micro avg       0.66      0.77      0.71      4650\n",
            "   macro avg       0.63      0.74      0.68      4650\n",
            "weighted avg       0.67      0.77      0.72      4650\n",
            " samples avg       0.66      0.74      0.66      4650\n",
            "\n",
            "\n",
            "F1 Score with macro average: 0.6778615200702238\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}