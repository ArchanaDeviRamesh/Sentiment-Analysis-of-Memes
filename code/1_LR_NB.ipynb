{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Group5_LR_NB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pIVvj3ayBJU",
        "outputId": "ad943143-1d05-4a4d-bdcd-f898fd8733f1"
      },
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report\n",
        "from sklearn.metrics import recall_score,precision_score,precision_recall_fscore_support\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVKHrTRxy2R1",
        "outputId": "3f29aa46-61ea-42c6-8d99-c149805da866"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6x3lJEJzWzN"
      },
      "source": [
        "train_data = '/content/gdrive/MyDrive/NLP_Project/labels.csv'\n",
        "test_data = '/content/gdrive/MyDrive/NLP_Project/2000_testdata.csv'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSe0vFpXyicG"
      },
      "source": [
        "#basic preprocessing\n",
        "\n",
        "#removing punctuation\n",
        "def remove_punct_numbers(words):\n",
        "    return [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "# remove stop words\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "# lemmatize\n",
        "lematizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lematize(words):\n",
        "    words = [lematizer.lemmatize(word,pos=get_wordnet_pos(word)) for word in words]\n",
        "    #words = [lematizer.lemmatize(word,pos='v') for word in words]\n",
        "    return words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1a_l2VfykB0"
      },
      "source": [
        "#Preprocessing train data and test data \n",
        "\n",
        "def input_func(file, label):\n",
        "    #input_path = input('/content/labels.csv')\n",
        "    #input_path = input_path.replace (\"/\",\"//\")\n",
        "    df = pd.read_csv(file)\n",
        "    df = df.dropna()\n",
        "    df['processed'] = df[label].apply(lambda x:nltk.word_tokenize(x))\n",
        "    df['processed'] = df['processed'].apply(remove_punct_numbers)\n",
        "    df['processed'] = df['processed'].apply(remove_stopwords)\n",
        "    df['processed'] = df['processed'].apply(lematize)\n",
        "    return df\n",
        "\n",
        "df = input_func(train_data, 'text_ocr')\n",
        "df.to_csv('train_data_processed.csv')\n",
        "\n",
        "df = input_func(test_data, 'OCR_extracted_text')\n",
        "df.to_csv('test_data_processed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHr_Gy7E32k8"
      },
      "source": [
        "# TASK1 using LR/NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTbF2U-y3eAP"
      },
      "source": [
        "#reading csv file\n",
        "def read_corpus(file):\n",
        "  return pd.read_csv(file)\n",
        "  \n",
        "train_data = '/content/gdrive/MyDrive/NLP_Project/train_data_processed.csv'\n",
        "test_data = '/content/gdrive/MyDrive/NLP_Project/test_data_processed.csv'\n",
        "test_true = '/content/gdrive/MyDrive/NLP_Project/Test_Actual_Final.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "JMOj-73l4I-q",
        "outputId": "2a6b5524-2422-4c5b-b336-411daddf66bb"
      },
      "source": [
        "#Dataset containing true values \n",
        "\n",
        "true_df = read_corpus(test_true)\n",
        "#Extracting the first digit (1, 0 , -1) from Labels \n",
        "\n",
        "true_df['Sentiment'] = true_df['Labels'].str.split('_').str[0]\n",
        "true_df['Sentiment'] = true_df['Sentiment'].astype(int)\n",
        "true_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>1_1100_1100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>1_0100_0200</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>1_1110_1120</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>0_1111_1121</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>0_0000_0000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... Sentiment\n",
              "0           0  ...         1\n",
              "1           1  ...         1\n",
              "2           2  ...         1\n",
              "3           3  ...         0\n",
              "4           4  ...         0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "v4roJ7b24N2u",
        "outputId": "95ea842e-b46e-442f-8e05-83404e83b487"
      },
      "source": [
        "#Dataset containing the Train data\n",
        "train_df = read_corpus(train_data)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>image_name</th>\n",
              "      <th>text_ocr</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall_sentiment</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>image_1.jpg</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>hilarious</td>\n",
              "      <td>general</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>very_positive</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>image_2.jpeg</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>not_funny</td>\n",
              "      <td>general</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>very_positive</td>\n",
              "      <td>['best', 'yearchallenge', 'complete', 'less', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>image_3.JPG</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>not_sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>positive</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>image_4.png</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>twisted_meaning</td>\n",
              "      <td>very_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>positive</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>image_5.png</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>hilarious</td>\n",
              "      <td>very_twisted</td>\n",
              "      <td>very_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>neutral</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                          processed\n",
              "0           0  ...  ['look', 'friend', 'lightyear', 'sohalikut', '...\n",
              "1           1  ...  ['best', 'yearchallenge', 'complete', 'less', ...\n",
              "2           2  ...  ['sam', 'thorne', 'strippin', 'follow', 'follo...\n",
              "3           3  ...   ['year', 'challenge', 'sweet', 'dee', 'edition']\n",
              "4           4  ...  ['year', 'challenge', 'filter', 'hilarious', '...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmX64oeT4TB_",
        "outputId": "5a2f54d2-c465-4689-bcf6-253be8c9e22a"
      },
      "source": [
        "train_df['overall_sentiment'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive         3057\n",
              "neutral          2157\n",
              "very_positive    1001\n",
              "negative          469\n",
              "very_negative     146\n",
              "Name: overall_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAL8kEeU4Wha",
        "outputId": "4fe0ccad-5975-4f71-ba7f-0f2ea7605fa4"
      },
      "source": [
        "#converting categorical to numerical \n",
        "\n",
        "train_df['overall_sentiment'].replace({'very_negative': -1, 'negative': -1, 'neutral': 0, 'positive': 1, 'very_positive': 1}, inplace=True)\n",
        "train_df['overall_sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    4058\n",
              " 0    2157\n",
              "-1     615\n",
              "Name: overall_sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uSMeVAP4YPG",
        "outputId": "c5e8dda3-6310-4b75-b717-7fefafc68f7b"
      },
      "source": [
        "print(train_df.humour.value_counts())\n",
        "print(train_df.sarcasm.value_counts())\n",
        "print(train_df.offensive.value_counts())\n",
        "print(train_df.motivational.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "funny         2394\n",
            "very_funny    2176\n",
            "not_funny     1618\n",
            "hilarious      642\n",
            "Name: humour, dtype: int64\n",
            "general            3430\n",
            "not_sarcastic      1516\n",
            "twisted_meaning    1499\n",
            "very_twisted        385\n",
            "Name: sarcasm, dtype: int64\n",
            "not_offensive        2657\n",
            "slight               2536\n",
            "very_offensive       1424\n",
            "hateful_offensive     213\n",
            "Name: offensive, dtype: int64\n",
            "not_motivational    4421\n",
            "motivational        2409\n",
            "Name: motivational, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWDF7LhuQ9Lb"
      },
      "source": [
        "Task B:\n",
        "\n",
        "Not humorous => 0 and Humorous (funny, very funny, hilarious) => 1\n",
        "Not Sarcastic => 0 and Sarcastic (general, twisted meaning, very twisted) => 1\n",
        "Not offensive => 0 and Offensive (slight, very offensive, hateful offensive) => 1\n",
        "Not Motivational => 0 and Motivational => 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYWpjsH_4isW"
      },
      "source": [
        "\n",
        "train_df.humour[train_df['humour']!='not_funny']= 1\n",
        "train_df.humour[train_df['humour']=='not_funny']= 0\n",
        "train_df.sarcasm[train_df['sarcasm']!='not_sarcastic']= 1\n",
        "train_df.sarcasm[train_df['sarcasm']=='not_sarcastic']= 0\n",
        "train_df.offensive[train_df['offensive']!='not_offensive']= 1\n",
        "train_df.offensive[train_df['offensive']=='not_offensive']= 0\n",
        "train_df.motivational[train_df['motivational']!='not_motivational']= 1\n",
        "train_df.motivational[train_df['motivational']=='not_motivational']= 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "0DG8T2rH4p0j",
        "outputId": "421cefb1-45fb-4191-e4c1-498e8e8ae68e"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>image_name</th>\n",
              "      <th>text_ocr</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall_sentiment</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>image_1.jpg</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['look', 'friend', 'lightyear', 'sohalikut', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>image_2.jpeg</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['best', 'yearchallenge', 'complete', 'less', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>image_3.JPG</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>['sam', 'thorne', 'strippin', 'follow', 'follo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>image_4.png</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>['year', 'challenge', 'sweet', 'dee', 'edition']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>image_5.png</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>['year', 'challenge', 'filter', 'hilarious', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                          processed\n",
              "0           0  ...  ['look', 'friend', 'lightyear', 'sohalikut', '...\n",
              "1           1  ...  ['best', 'yearchallenge', 'complete', 'less', ...\n",
              "2           2  ...  ['sam', 'thorne', 'strippin', 'follow', 'follo...\n",
              "3           3  ...   ['year', 'challenge', 'sweet', 'dee', 'edition']\n",
              "4           4  ...  ['year', 'challenge', 'filter', 'hilarious', '...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldc8g3AZ4tx_",
        "outputId": "b029b89f-6c16-480f-cdb4-3395d3ba54f6"
      },
      "source": [
        "print(train_df.humour.value_counts())\n",
        "print(train_df.sarcasm.value_counts())\n",
        "print(train_df.offensive.value_counts())\n",
        "print(train_df.motivational.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    5212\n",
            "0    1618\n",
            "Name: humour, dtype: int64\n",
            "1    5314\n",
            "0    1516\n",
            "Name: sarcasm, dtype: int64\n",
            "1    4173\n",
            "0    2657\n",
            "Name: offensive, dtype: int64\n",
            "0    4421\n",
            "1    2409\n",
            "Name: motivational, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "txgbq2Oq4xTg",
        "outputId": "f67b2676-ae23-47da-a960-302cd1350b79"
      },
      "source": [
        "#Dataset containing the processed text of test data\n",
        "\n",
        "test_df = read_corpus(test_data)\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Image_URL</th>\n",
              "      <th>OCR_extracted_text</th>\n",
              "      <th>corrected_text</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>https://gtmemes.com/wp-content/uploads/2019/03...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "      <td>['magician', 'walk', 'water', 'chuck', 'norris...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>https://i.imgur.com/NDBB96K.png</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "      <td>['one', 'million', 'dollar', 'make', 'imgur']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>https://media0ch-a.akamaihd.net/83/96/9e457d63...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "      <td>['mom', 'friend', 'sleep', 'mom', 'fine', 'boy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>http://politicalmemes.com/wp-content/uploads/2...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "      <td>['guy', 'inherit', 'mess', 'whine', 'foxed', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>https://pics.me.me/threat-kim-jong-un-allegedl...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "      <td>['threat', 'kim', 'jong', 'un', 'allegedly', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                          processed\n",
              "0           0  ...  ['magician', 'walk', 'water', 'chuck', 'norris...\n",
              "1           1  ...      ['one', 'million', 'dollar', 'make', 'imgur']\n",
              "2           2  ...  ['mom', 'friend', 'sleep', 'mom', 'fine', 'boy...\n",
              "3           3  ...  ['guy', 'inherit', 'mess', 'whine', 'foxed', '...\n",
              "4           4  ...  ['threat', 'kim', 'jong', 'un', 'allegedly', '...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftqCUhtR5Knk"
      },
      "source": [
        "#required data\n",
        "\n",
        "#for training\n",
        "train_text = train_df['processed']  #X_train\n",
        "sentiment = train_df['overall_sentiment'] #Y_train\n",
        "\n",
        "#for testing\n",
        "test_text = test_df['processed'] #X_test\n",
        "\n",
        "#for predicting\n",
        "actual_test_sentiment = true_df['Sentiment'] #Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOCun6KH5Od8",
        "outputId": "ddd70a7a-6afa-4b09-b1dd-282d49a8a039"
      },
      "source": [
        "#1. Count vectorizer for bag of words\n",
        "\n",
        "cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "cv_train = cv.fit_transform(train_text)\n",
        "cv_test = cv.transform(test_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW train: (6830, 68600)\n",
            "BOW test: (1859, 68600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm_R4TGJ5hDo"
      },
      "source": [
        "#2. using TFIDF\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "cv_train = vectorizer.fit_transform(train_text)    \n",
        "cv_test = vectorizer.transform(test_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSlHoYPn5nFc",
        "outputId": "2bad6e2b-d5b8-456b-d836-8ce19b1d35a6"
      },
      "source": [
        "#training the model using LR\n",
        "model = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
        "\n",
        "#model = MultinomialNB()\n",
        "\n",
        "#Fitting the model for Bag of words\n",
        "model.fit(cv_train, sentiment)\n",
        "\n",
        "print(\"\\n Training Accuracy: \", model.score(cv_train, sentiment)*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Training Accuracy:  95.2415812591508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onZTtkqB6HBJ",
        "outputId": "108e3bf6-bca7-49a3-d949-ca52b179d2b7"
      },
      "source": [
        "#making prediction\n",
        "predicted_test_sentiment = model.predict(cv_test)\n",
        "\n",
        "#calculating accuracy\n",
        "\n",
        "print(' Test accuracy is {}'.format(accuracy_score(actual_test_sentiment, predicted_test_sentiment) * 100))\n",
        "print(\" F1 Score: {:.2f}\".format(f1_score(actual_test_sentiment, predicted_test_sentiment, average='macro') * 100))\n",
        "print(\" Precision Score: {:.2f}\".format(precision_score(actual_test_sentiment, predicted_test_sentiment, average='macro') * 100))\n",
        "print(\" Recall Score: {:.2f}\".format(recall_score(actual_test_sentiment, predicted_test_sentiment, average='macro') * 100))\n",
        "\n",
        "report = classification_report(actual_test_sentiment, predicted_test_sentiment)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Test accuracy is 58.095750403442715\n",
            " F1 Score: 26.30\n",
            " Precision Score: 30.41\n",
            " Recall Score: 33.18\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00       172\n",
            "           0       0.32      0.03      0.06       586\n",
            "           1       0.59      0.96      0.73      1101\n",
            "\n",
            "    accuracy                           0.58      1859\n",
            "   macro avg       0.30      0.33      0.26      1859\n",
            "weighted avg       0.45      0.58      0.45      1859\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aTRy7G7ZRz"
      },
      "source": [
        "# TASK2 using LR/NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42utHlM7cUr"
      },
      "source": [
        "## Import The Dataset  \n",
        "train_df=pd.read_csv(train_data)\n",
        "test_df,true_df=pd.read_csv(test_data),pd.read_csv(test_true)\n",
        "\n",
        "train_df.humour[train_df['humour']!='not_funny']='1'\n",
        "train_df.humour[train_df['humour']=='not_funny']='0'\n",
        "train_df.sarcasm[train_df['sarcasm']!='not_sarcastic']='1'\n",
        "train_df.sarcasm[train_df['sarcasm']=='not_sarcastic']='0'\n",
        "train_df.offensive[train_df['offensive']!='not_offensive']='1'\n",
        "train_df.offensive[train_df['offensive']=='not_offensive']='0'\n",
        "train_df.motivational[train_df['motivational']!='not_motivational']='1'\n",
        "train_df.motivational[train_df['motivational']=='not_motivational']='0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoWm3M-6M-h"
      },
      "source": [
        "def Multilabel(classifier,train_df,test_df,true_df):\n",
        "    X_train=train_df['processed']\n",
        "    X_test=test_df['processed']\n",
        "    \n",
        "    #### Create Bag of words \n",
        "    cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "    cv_train = cv.fit_transform(X_train)\n",
        "    cv_test = cv.transform(X_test)\n",
        "    \n",
        "    if classifier.lower()=='lr':\n",
        "        # Using pipeline for applying logistic regression and one vs rest classifier\n",
        "        model_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
        "            ])\n",
        "    elif classifier.lower()=='nb':\n",
        "        # Using pipeline for applying Naive Bayes and one vs rest classifier\n",
        "        model_pipeline  = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(), n_jobs=-1)),\n",
        "            ])\n",
        "        \n",
        "    \n",
        "    categories={'humour':0,'sarcasm':1,'offensive':2,'motivational':3}\n",
        "    \n",
        "\n",
        "    for category in categories.keys():\n",
        "        print('*Processing {} category...*'.format(category))\n",
        "    \n",
        "        # Training model pipeline on train data\n",
        "        model_pipeline.fit(cv_train, train_df[category])\n",
        "        \n",
        "        # Applying k-Fold Cross Validation\n",
        "        accuracies = cross_val_score(model_pipeline, X = cv_train, y = train_df[category], cv=KFold(n_splits=5))\n",
        "        print(\"\\n Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "        print(\" Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
        "        \n",
        "        # calculating test accuracy\n",
        "        predicted = model_pipeline.predict(cv_test)\n",
        "        y_test=true_df['Labels'].str.split('_').str[1].str[categories[category]]\n",
        "        report = classification_report(y_test, predicted)\n",
        "        \n",
        "        print(report)\n",
        "        print(' Test accuracy is {}'.format(accuracy_score(y_test,predicted)))\n",
        "        print(\" F1 Score: {:.2f}\".format(f1_score(y_test, predicted, average='macro') * 100))\n",
        "        print(\" Precision Score: {:.2f}\".format(precision_score(y_test, predicted, average='weighted') * 100))\n",
        "        print(\" Recall Score: {:.2f}\".format(recall_score(y_test, predicted, average='weighted') * 100))\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSrEwbJb6Q7D",
        "outputId": "a60db58a-163f-4466-acb5-366f42953c54"
      },
      "source": [
        "Multilabel('LR',train_df,test_df,true_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*Processing humour category...*\n",
            "\n",
            " Accuracy: 76.31 %\n",
            " Standard Deviation: 1.28 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.18      0.00      0.01       443\n",
            "           1       0.76      0.99      0.86      1416\n",
            "\n",
            "    accuracy                           0.76      1859\n",
            "   macro avg       0.47      0.50      0.44      1859\n",
            "weighted avg       0.62      0.76      0.66      1859\n",
            "\n",
            " Test accuracy is 0.7579343733189887\n",
            " F1 Score: 43.55\n",
            " Precision Score: 62.33\n",
            " Recall Score: 75.79\n",
            "\n",
            "\n",
            "*Processing sarcasm category...*\n",
            "\n",
            " Accuracy: 77.80 %\n",
            " Standard Deviation: 0.87 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.00      0.00       418\n",
            "           1       0.77      0.99      0.87      1441\n",
            "\n",
            "    accuracy                           0.77      1859\n",
            "   macro avg       0.44      0.50      0.44      1859\n",
            "weighted avg       0.63      0.77      0.68      1859\n",
            "\n",
            " Test accuracy is 0.771382463690156\n",
            " F1 Score: 43.78\n",
            " Precision Score: 62.54\n",
            " Recall Score: 77.14\n",
            "\n",
            "\n",
            "*Processing offensive category...*\n",
            "\n",
            " Accuracy: 61.10 %\n",
            " Standard Deviation: 1.81 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.04      0.08       700\n",
            "           1       0.62      0.96      0.76      1159\n",
            "\n",
            "    accuracy                           0.62      1859\n",
            "   macro avg       0.51      0.50      0.42      1859\n",
            "weighted avg       0.54      0.62      0.50      1859\n",
            "\n",
            " Test accuracy is 0.615922538999462\n",
            " F1 Score: 41.64\n",
            " Precision Score: 54.10\n",
            " Recall Score: 61.59\n",
            "\n",
            "\n",
            "*Processing motivational category...*\n",
            "\n",
            " Accuracy: 64.73 %\n",
            " Standard Deviation: 1.53 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.98      0.77      1176\n",
            "           1       0.44      0.02      0.04       683\n",
            "\n",
            "    accuracy                           0.63      1859\n",
            "   macro avg       0.54      0.50      0.41      1859\n",
            "weighted avg       0.56      0.63      0.50      1859\n",
            "\n",
            " Test accuracy is 0.6304464766003227\n",
            " F1 Score: 40.52\n",
            " Precision Score: 56.17\n",
            " Recall Score: 63.04\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjbuPqED8YnH",
        "outputId": "29cebac9-c7d5-4bf2-9d0a-ee4155e925eb"
      },
      "source": [
        "Multilabel('NB',train_df,test_df,true_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*Processing humour category...*\n",
            "\n",
            " Accuracy: 39.55 %\n",
            " Standard Deviation: 0.82 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.06      0.10       443\n",
            "           1       0.76      0.94      0.84      1416\n",
            "\n",
            "    accuracy                           0.73      1859\n",
            "   macro avg       0.50      0.50      0.47      1859\n",
            "weighted avg       0.64      0.73      0.66      1859\n",
            "\n",
            " Test accuracy is 0.7310381925766541\n",
            " F1 Score: 66.46\n",
            " Precision Score: 63.84\n",
            " Recall Score: 73.10\n",
            "\n",
            "\n",
            "*Processing sarcasm category...*\n",
            "\n",
            " Accuracy: 36.08 %\n",
            " Standard Deviation: 0.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.06      0.09       418\n",
            "           1       0.78      0.95      0.86      1441\n",
            "\n",
            "    accuracy                           0.75      1859\n",
            "   macro avg       0.51      0.50      0.47      1859\n",
            "weighted avg       0.66      0.75      0.68      1859\n",
            "\n",
            " Test accuracy is 0.750403442711135\n",
            " F1 Score: 68.33\n",
            " Precision Score: 65.81\n",
            " Recall Score: 75.04\n",
            "\n",
            "\n",
            "*Processing offensive category...*\n",
            "\n",
            " Accuracy: 44.90 %\n",
            " Standard Deviation: 1.45 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.20      0.26       700\n",
            "           1       0.62      0.78      0.69      1159\n",
            "\n",
            "    accuracy                           0.56      1859\n",
            "   macro avg       0.49      0.49      0.48      1859\n",
            "weighted avg       0.52      0.56      0.53      1859\n",
            "\n",
            " Test accuracy is 0.5648197955890264\n",
            " F1 Score: 52.92\n",
            " Precision Score: 52.22\n",
            " Recall Score: 56.48\n",
            "\n",
            "\n",
            "*Processing motivational category...*\n",
            "\n",
            " Accuracy: 42.28 %\n",
            " Standard Deviation: 2.12 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.84      0.73      1176\n",
            "           1       0.40      0.18      0.25       683\n",
            "\n",
            "    accuracy                           0.60      1859\n",
            "   macro avg       0.52      0.51      0.49      1859\n",
            "weighted avg       0.55      0.60      0.55      1859\n",
            "\n",
            " Test accuracy is 0.5981710597095212\n",
            " F1 Score: 54.98\n",
            " Precision Score: 54.93\n",
            " Recall Score: 59.82\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}